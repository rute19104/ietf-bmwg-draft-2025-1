<?xml version="1.0" encoding="utf-8"?>
<!-- 
     draft-rfcxml-general-template-standard-00
  
     This template includes examples of the most commonly used features of RFCXML with comments 
     explaining how to customise them. This template can be quickly turned into an I-D by editing 
     the examples provided. Look for [REPLACE], [REPLACE/DELETE], [CHECK] and edit accordingly.
     Note - 'DELETE' means delete the element or attribute, not just the contents.
     
     Documentation is at https://authors.ietf.org/en/templates-and-schemas
-->
<?xml-model href="rfc7991bis.rnc"?>  <!-- Required for schema validation and schema-aware editing -->
<!-- <?xml-stylesheet type="text/xsl" href="rfc2629.xslt" ?> -->
<!-- This third-party XSLT can be enabled for direct transformations in XML processors, including most browsers -->


<!DOCTYPE rfc [
  <!ENTITY nbsp    "&#160;">
  <!ENTITY zwsp   "&#8203;">
  <!ENTITY nbhy   "&#8209;">
  <!ENTITY wj     "&#8288;">
]>
<!-- If further character entities are required then they should be added to the DOCTYPE above.
     Use of an external entity file is not recommended. -->

<rfc
  xmlns:xi="http://www.w3.org/2001/XInclude"
  category="info"
  docName="draft-samizadeh-bmwg-cni-benchmarking-00"
  ipr="trust200902"
  obsoletes=""
  updates=""
  submissionType="IETF"
  xml:lang="en"
  version="3">
  
<!-- [REPLACE] 
       * docName with name of your draft
     [CHECK] 
       * category should be one of std, bcp, info, exp, historic
       * ipr should be one of trust200902, noModificationTrust200902, noDerivativesTrust200902, pre5378Trust200902
       * updates can be an RFC number as NNNN
       * obsoletes can be an RFC number as NNNN 
-->

  <front>
    <title abbrev="CNI Benchmarking"></title>
    <!--  [REPLACE/DELETE] abbrev. The abbreviated title is required if the full title is longer than 39 characters -->

    <seriesInfo name="Internet-Draft" value="draft-samizadeh-bmwg-cni-benchmarking-00"/>
   
    <author fullname="Tina Samizadeh" initials="T." surname="Samizadeh">
      <!-- [CHECK]
             * initials should not include an initial for the surname
             * role="editor" is optional -->
    <!-- Can have more than one author -->
      
    <!-- all of the following elements are optional -->
      <organization>fortiss GmbH</organization>
      <address>
        <postal>
          <!-- Reorder these if your country does things differently -->
          <street>Guerickestr. 25</street>
          <city>Munich</city>
          <code>80805</code>
          <country>DE</country>
          <!-- Uses two letter country code -->
        </postal>        
        <email>sofia@fortiss.org</email>  
        <!-- Can have more than one <email> element -->
        <uri>www.rutesofia.com</uri>
      </address>
    </author>

    <author fullname="George Koukis" initials="G." surname="Koukis">
      <!-- [CHECK]
             * initials should not include an initial for the surname
             * role="editor" is optional -->
    <!-- Can have more than one author -->
      
    <!-- all of the following elements are optional -->
      <organization>ATHENA RC</organization>
      <address>
        <postal>
          <!-- Reorder these if your country does things differently -->
          <street>University Campus South Entrance</street>
          <city>Xanthi</city>
          <code>67100</code>
          <country>Greece</country>
          <!-- Uses two letter country code -->
        </postal>        
        <email>George.Koukis@athenarc.gr</email>  
        <!-- Can have more than one <email> element -->
        <uri></uri>
      </address>
    </author>
    
     <author fullname="Rute C. Sofia" initials="R." surname="C. Sofia">
      <!-- [CHECK]
             * initials should not include an initial for the surname
             * role="editor" is optional -->
    <!-- Can have more than one author -->
      
    <!-- all of the following elements are optional -->
      <organization>fortiss GmbH</organization>
      <address>
        <postal>
          <!-- Reorder these if your country does things differently -->
          <street>Guerickestr. 25</street>
          <city>Munich</city>
          <code>80805</code>
          <country>DE</country>
          <!-- Uses two letter country code -->
        </postal>        
        <email>sofia@fortiss.org</email>  
        <!-- Can have more than one <email> element -->
        <uri>www.rutesofia.com</uri>
      </address>
    </author>

   
     <date day="07" month="July" year="2025" />
    <!-- On draft subbmission:
         * If only the current year is specified, the current day and month will be used.
         * If the month and year are both specified and are the current ones, the current day will
           be used
         * If the year is not the current one, it is necessary to specify at least a month and day="1" will be used.
    -->


    <area>Operations and Management Area</area>
    <workgroup>Benchmarking Methodology Working Group</workgroup>
    <keyword>Internet-Draft</keyword>
    <keyword>CNI</keyword>
    <keyword>SDN</keyword>
    <keyword>Edge-Cloud</keyword>

    <abstract>
      <t> 
      This document investigates benchmarking methodologies for Kubernetes Container Network Interfaces (CNIs) in Edge-to-Cloud environments. It outlines performance, scalability, and observability metrics relevant to CNIs, and aligns with the goals of the IETF Benchmarking Methodology Working Group (BMWG). The document surveys current practices, introduces repeatable testing frameworks (e.g., CODEF), and proposes a path toward standardized, vendor-neutral benchmarking procedures for CNIs in microservice-oriented, distributed infrastructures.
 
     </t>
   </abstract>
    
  </front>

  <middle>
    
    <section>
      <name>Introduction</name>
      <t>     
     This document presents an initial exploration of benchmarking methodologies for Kubernetes Container Network Interfaces (CNIs) in Edge-to-Cloud environments. It evaluates the performance characteristics of common Kubernetes networking plugins—such as Multus, Calico, Cilium, and Flannel—within the scope of container orchestration platforms. The draft aims to align with the principles of the IETF Benchmarking Methodology Working Group (BMWG) by proposing a framework for repeatable, comparable, and vendor-neutral benchmarking of CNIs. Emphasis is placed on performance aspects relevant to Software Defined Networking (SDN) architectures and distributed deployments. The goal is to inform the development of formal benchmarking procedures tailored to CNIs in heterogeneous infrastructure scenarios.
</t>
    </section>
      
      <section>
        <name>Requirements Language</name>
        <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL",
          "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT
          RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be
          interpreted as described in BCP 14 <xref target="RFC2119"/>
          <xref target="RFC8174"/> when, and only when, they appear in
          all capitals, as shown here.</t>
      </section>
      <!-- [CHECK] The 'Requirements Language' section is optional -->

<section anchor="statement">
 <name>Problem Statement and Alignment with BMWG Goals</name>
<t>
BMWG proposes and debates methodologies and metrics to evaluate performance characteristics of networking devices and systems in a repeatable, vendor-neutral, and interoperable manner. While multiple Kubernetes CNI solutions exist and are critical to Kubernetes networking and as such, to improve the alignment for telco-cloud networking solutions, there is currently no standardized methodology for benchmarking their performance, resource utilization, or behavior under varying operational conditions. The absence of such standards leads to non-reproducible, vendor-specific results that are difficult to compare or rely on for deployment decisions in edge-cloud contexts. 

This document aligns with BMWG goals by proposing benchmarking considerations for Kubernetes Container Network Interface (CNI) plugins that adhere to the following principles:</t>

<ul>
  <li>Repeatability and Reproducibility: The draft emphasizes deterministic test environments by leveraging clean-slate container orchestration through automation frameworks such as the experimental open-source Cognitive Decentralised Edge Cloud (CODECO)<xref target="codeco_d10"/>  Experimentation Framework (CODEF) <xref target="codef"/>. Test cases are repeatable across deployments, and variability in underlying infrastructure (e.g., bare metal vs. virtualized environments) is explicitly documented to preserve reproducibility, following BMWG best practices <xref target="RFC2544"/><xref target="RFC7312"/>.</li>
   <li>Vendor-Neutral Evaluation: The proposed approach includes a diverse set of CNIs from multiple vendors and open-source communities, avoiding platform-specific optimizations. CNIs are evaluated under the same environmental and workload conditions to provide fair comparisons, consistent with BMWG's commitment to vendor-agnostic test procedures.</li>
   <li>Metrics-Based Assessment: The document adopts classical benchmarking metrics including latency, throughput, jitter, and resource consumption (CPU, memory), extending them with CNI-relevant attributes such as pod network initialization time and observability overhead. These metrics are aligned with performance evaluation goals outlined in <xref target="RFC1242"/><xref target="RFC2285"/>, and more recent benchmarking efforts for virtualized environments <xref target="RFC8172"/></li>
   <li>Applicability to Emerging Architectures: the targeted environment includes Edge-to-Cloud deployments, which represent modern distributed systems architectures. While BMWG has historically focused solely on network appliances, this work extends those principles into the networking aspects of containerized and software-defined infrastructures, continuing the evolution of benchmarking methods to address dynamic, microservice-based platforms.</li>
   <li>Traffic and Control Plane Separation: Following BMWG precedent (e.g., <xref target="RFC6808"/>), the methodology distinguishes between control-plane operations (e.g., pod deployment and CNI setup latency) and data-plane behavior (e.g., packet forwarding performance), allowing comprehensive benchmarking of CNIs across operational dimensions.</li>
   <li>Scalability and Stress Testing: The methodology incorporates stress and scalability scenarios, consistent with goals in <xref target="RFC8239"/>, to uncover performance degradation points and assess operational resilience of CNIs under heavy load and fault conditions.</li>

</ul>
<t>
This alignment ensures that future extensions of this document toward a formal benchmarking specification can be scoped within the BMWG charter and contribute to standardized practices for container network evaluation.
</t>

<section anchor="metrics-scope">
 <name>Scope of Metrics</name>
 <t>
 The core benchmarking metrics in this document such as latency, throughput, jitter, packet loss, and pod lifecycle time are aligned with BMWG practices. 
 
 Additional metrics such as resource usage, energy efficiency, and operational ease are included to reflect real-world operator concerns but are considered informational and outside the core BMWG scope.</t>
</section>
</section>

<section anchor="definitions">
  <name>Definitions</name>

  <dl newline="true">
        <!-- Omit newline="true" if you want each definition to start on the same line as the corresponding term -->
        <dt>Latency:</dt>
        <dd> The time interval between the sending of the first bit of a packet from the source and the reception of the last bit at the destination <xref target="RFC1242"/>. </dd>
        <dt>Throughput</dt>
        <dd>The average rate of successful data delivery over a communication channel, measured in bits per second (bps), as defined in <xref target="RFC2544"/>.</dd>
        <dt>Packet loss rate</dt>
        <dd>  The percentage of packets lost during transmission, typically due to congestion, buffer overflows, or link errors <xref target="RFC1242"/>.</dd>
        <dt>Jitter</dt>
        <dd> The variation in latency between packets in a stream, typically measured as the standard deviation of delay <xref target="RFC5481"/>.</dd>
         <dt>Pod Initialization Time</dt>
        <dd> The elapsed time between the submission of a pod specification and the point at which the pod is fully network-ready.</dd>
          <dt>Pod Deletion Time</dt>
        <dd> The time required for a pod to be gracefully terminated and its networking stack (CNI DEL) torn down.</dd>
      </dl>
</section>

    
    
<section anchor="CNI">
<name>Container Network Interfaces in Kubernetes</name>
<t>
The CNI is a set of specifications and libraries that defines how container runtimes should configure network interfaces for containers and manage their connectivity. CNIs are essential components in Kubernetes to implement the Kubernetes network model <xref target="K8s-netw-model"/>, providing a standardized and container runtime agnostic way to configure network interfaces within containers and networks. In practice, they function as the intermediary layer that connects the Kubernetes control plane to a Kubernetes cluster/multi-cluster underlying network infrastructure. From a networking perspective, since a pod or container lacks network connectivity when first created, Kubernetes relies on a CNI plugin to:</t>
<ul>
<li>attach a virtual network interface (e.g., a veth) inside the container’s namespace,</li>
<li>link that interface to the host’s networking stack,</li>
<li>assign an IP address,</li>
<li>set up network policies for isolation and</li>
<li>install the appropriate routing information according to the cluster’s IPAM strategy.</li>
</ul>
<t>
CNIs enable seamless communication between micro-services (Kubernetes pods) in the cluster using pod IP addresses without NAT, with external networks and the outside world and can be categorized into four main categories based on their functional role and deployment scope:</t>
<ul>
<li>Standard Third-Party Plugins or Normal Non-Acceleration Networking Models <xref target="ietf-bmwg-07"/>: This category includes container networking solutions that rely on a single CNI plugin as the default network 
  provider. These CNIs are typically installed via Kubernetes manifests (e.g., YAML files) and do not rely on external acceleration hardware. They implement pod-to-pod and pod-to-service networking within the Kubernetes cluster using standard Linux networking features, including iptables, nftables, eBPF, and tunneling mechanisms.</li>
</ul>

<ul>
<li>Antrea <xref target="antrea"/>: Built on Open vSwitch (OVS), Antrea provides a feature-rich datapath with support for OpenFlow rules and Kubernetes-native policies. An optional eBPF datapath is also available to improve performance and enhance observability.</li>
<li>Calico <xref target="calico"/>: Uses iptables or nftables as the default datapath and optionally supports an eBPF mode that can replace kube-proxy. It offers flexible policy enforcement, supports tunneling via VXLAN, IP-in-IP, WireGuard, and emphasizes scalability and performance.</li>
<li>Cillium <xref target="cilium"/>: Implements a pure eBPF/XDP datapath that bypasses iptables entirely. It provides built-in L3–L7 observability (via the Hubble observability engine), native Kubernetes NetworkPolicy enforcement, identity-based security, and encryption support.</li>
<li>Flannel <xref target="flannel"/>: A lightweight plugin that establishes an overlay network using VXLAN or host-gw modes. Flannel is often used with Calico in a hybrid setup (e.g., Canal) where Flannel provides basic connectivity and Calico handles policy enforcement.</li>
<li>Kube-OVN <xref target="kube-ovn"/>: A cloud-native SDN built atop Open Virtual Network (OVN). Kube-OVN supports L2/L3 logical networking, IPv6/dual-stack, QoS policies, and Kubernetes CRDs for tenant isolation and IP management.</li>
<li>Kube-Router <xref target="kube-router"/>: Integrates with the Linux kernel to provide routing via BGP, service proxying via IPVS, and network policy enforcement using iptables. It prioritizes simplicity and minimal resource overhead.</li>
<li>Weave Net <xref target="weavenet"/>: Implements a peer-to-peer mesh overlay with automatic node discovery and encrypted communications. Designed for multi-cloud and edge scenarios, Weave Net was archived in June 2024 but remains referenced in historical deployments.</li>
</ul>
<t>
These design choices SHOULD be considered in CNI performance benchmarking across varied workloads and deployment scenarios, while they SHOULD be evaluated within the context of the full containerized infrastructure to reflect real-world behavior.</t>
</section>

<section anchor="CNI-BenchP">
<name>CNI Benchmarking Key Aspects</name>
<t>
While several performance-benchmarking suites are already available from CNI providers <xref target="cilium-bench"/>, the open-source community <xref target="TNSM21-cni"/>, and also in the IETF BMWG <xref target="ietf-bmwg-07"/>, a comprehensive CNI evaluation SHOULD incorporate relevant performance metrics, scalability aspects and identify bottlenecks. This section provides a view on relevant aspects to ensure reliable and replicable performance evaluation, considering aspects that are relevant from a telco-cloud perspective.
</t>
 
<section anchor="CNI-Performance">
<name>Relevant Performance Metrics </name>
<t>
Considering the architecture of microservice-based applications, microservices may interact with each other and external services. Having containerized applications and orchestration platforms like Kubernetes, there is a continuous need to address communication and networking as Kubernetes doesn't handle networking itself. Moreover, communication between containers is extremely important to meet QoS requirements of applications. To evaluate the performance of CNIs there are several metrics that should be taken into account including network throughput, end-to-end latency, pod setup and deletion times, CPU and Memory utilization, etc.
</t>

<section anchor="CNI-QoS">
<name>Quality of Service</name>
<t> Benchmarking Quality of Service (QoS) for CNI plugins typically focuses on traditional performance metrics such as one-way latency, round-trip delay, packet loss, jitter, and achievable data rates under varied network conditions. These metrics are fundamental to assessing the efficiency and responsiveness of a CNI in both intra-cluster and inter-cluster communication scenarios. To ensure comprehensive evaluation, the benchmarking methodology SHOULD include tests using multiple transport protocols, primarily TCP and UDP. This is essential, as CNI plugins may exhibit significantly different performance profiles depending on the protocol type due to variations in connection setup, flow control, and packet processing overhead.  For TCP, two key test modes are RECOMMENDED:</t>
 <ul > 
 <li>TCP_RR (Request/Response): Measures the rate at which application-layer request/response pairs can be exchanged over a persistent TCP connection. This reflects transaction latency under connection reuse scenarios.</li>
  <li>TCP_CRR (Connect/Request/Response): Assesses the rate at which new TCP connections can be established, used for a request/response exchange, and torn down. This test exposes connection setup overhead and potential scalability bottlenecks.</li>
 </ul> 
<t> For UDP, the benchmark SHOULD include  UDP_RR testing, which captures round-trip time (RTT), latency variation (jitter), and packet loss characteristics under lightweight, connectionless exchanges. 

In all tests, the benchmarking suite MUST include a representative range of payload sizes, including at least 64 bytes, 512 bytes, and 1500 bytes. If supported by the underlying network and CNI plugin, jumbo frames (e.g., MTU > 1500 bytes) SHOULD also be tested to expose potential fragmentation penalties and their impact on latency, jitter, and throughput. </t>
</section>

<section anchor="CNI-QoE">
<name>Quality of Experience</name>
<t>

Quality of Experience (QoE) benchmarking for Container Network Interface (CNI) plugins extends beyond conventional network performance metrics such as latency and throughput. It focuses on assessing operational usability, deployment efficiency, and portability, i.e., factors that directly affect the user experience of platform administrators, DevOps engineers, and developers.  For instance, time to deploy or configure the CNI, ease of troubleshooting, and impact of the CNI on application performance are examples of QoE parameters. 

Key QoE indicators MAY include:  </t>
<ul>
<li>Deployment time, the time required to install or upgrade a CNI plugin using declarative tooling (e.g., Helm charts, YAML manifests).</li>
<li>Configuration simplicity, the extent to which configuration is automated, validated, and integrated with Kubernetes-native workflows.</li>
<li>Troubleshooting tooling, the presence of purpose-built CLI utilities that simplify diagnostics, expose internal CNI state, and reduce reliance on low-level log inspection or manual kubectl commands.</li> 
</ul> 
<t> For example, CNI-specific command-line interfaces such as cillium and calicoctl provide capabilities such as one-command installation, real-time policy and connectivity status, and automated diagnostics. The cillium status --verbose command provides IPAM allocations, agent health, and datapath metrics, while the calicoctl node diags generates complete diagnostic bundles for analysis. 

CNI integration with Kubernetes distribution CLIs (e.g., k3s, MicroK8s) further improves QoE by streamlining lifecycle operations. For instance, MicroK8s leverages snap-based add-ons that can enable or disable CNIs via a single command, reducing complexity and configuration drift.Although these attributes are not part of the core benchmarking metrics defined by BMWG, their inclusion is RECOMMENDED to reflect practical DevOps concerns and enhance the applicability of CNI benchmarking results in production environments. </t>

</section>

<section anchor="CNI-deployment-deletion">
<name>Deployment and Deletion Times</name>
<t>
Deployment and deletion times are key metrics within the cluster lifecycle, capturing the control-plane overhead that a CNI introduces. At pod level they measure how long the kubelet needs to create a new network stack (CNI ADD) or tear it down (CNI DEL), while at cluster level they show how quickly a CNI DaemonSet can roll during upgrades. Therefore, a benchmarking methodology SHOULD quantify both operations with clearly defined metrics.
</t>
</section>

<section anchor="CNI-CPU">
<name>Node Resources: CPU, Memory</name>
<t> The CPU and memory footprint of a Container Network Interface (CNI) plugin has substantial implications for workload density and system scalability, especially in resource-constrained or heterogeneous environments. In modern Edge-to-Cloud deployments—often comprising diverse processor architectures (e.g., ARM64, AMD64) and variable memory constraints—resource efficiency is critical to maximizing node utilization and sustaining performance. 

The architectural design of a CNI directly affects its resource profile. CNIs with extensive feature sets and complex data-plane capabilities—such as policy enforcement, encryption, overlay encapsulation (e.g., VXLAN, IP-in-IP), or eBPF/XDP acceleration—tend to exhibit higher CPU and memory consumption. For example, CNIs that perform user-space packet processing typically incur higher overhead, as each packet traverses the kernel-user boundary multiple times, resulting in increased CPU cycles and memory copies <xref target="RFC8172"/>. In contrast, in-kernel eBPF-based processing can reduce such overhead by executing directly in the Linux kernel <xref target="RFC9315"/>. 

In cloud-native deployments, CNIs that manage external interfaces (e.g., Elastic Network Interfaces (ENIs) in public cloud environments) may also introduce persistent memory usage due to API caching, state tracking, and metadata management <xref target="aws-vpc-cni-docs"/>. These variabilities are further amplified under dynamic workloads. It is frequently observed that a CNI optimized for high-throughput TCP bulk traffic may perform suboptimally under UDP-heavy traffic, high pod churn, or policy-intensive workloads. These behavioral differences necessitate a systematic and multi-dimensional benchmarking approach. 

 Accordingly, a robust benchmarking methodology SHOULD assess each CNI under at least three operating states: idle, low-traffic (and low load), high traffic (and high load). Such profiling enables the identification of baseline resource usage, saturation thresholds, and degradation points ("performance peaks"). Measurements SHOULD be taken at both the node level (e.g., using Prometheus <xref target="prometheus-docs"/>) and at the container or pod level (e.g., using cAdvisor <xref target="cadvisor-docs"/>). These practices are consistent with recommendations for virtualized and cloud-native benchmarking environments as described in <xref target="RFC8172"/>. </t>
</section>
</section>


<section anchor="CNI-interoperability">
<name>Interoperability and Scalability</name>
<t>
To ensure comprehensive benchmarking coverage, scalability and stress-testing phases SHOULD be incorporated into the evaluation methodology. These phases are essential to identify the performance ceilings of a given CNI plugin and to assess its behavior under saturation conditions, including whether key observability features remain functional. Such assessments are consistent with guidance outlined in <xref target="RFC8239"/> and extend benchmarking scope beyond nominal operation to failure and recovery modes.

Stress tests SHOULD simulate high-load scenarios by concurrently scaling multiple Kubernetes components. This includes initiating rapid pod-creation bursts, deploying multiple concurrent services and network policies, and triggering controlled resource exhaustion events (e.g., CPU throttling, memory pressure, disk I/O contention). Furthermore, synthetic network impairments—such as increased latency, jitter, or packet loss—SHOULD be introduced using tools like <xref target="tc-netem"/> to assess the CNI’s robustness under adverse network conditions.

The use of orchestration tools such as Kube-Burner <xref target="kube-burner"/> and chaos engineering frameworks (e.g., Chaos Mesh or Litmus) is RECOMMENDED to coordinate scalable and repeatable test scenarios. Network performance metrics during stress tests MAY be collected with traffic generators such as iperf3, netperf, or k6 <xref target="iperf3"/> <xref target="k6"/>. Benchmark results SHOULD include degradation thresholds, error rates, recovery latency, and metrics export consistency under stress to support the evaluation of CNI resilience and operational observability.
</t>
</section>

<section anchor="CNI-observability">
<name>Observability and Bottleneck Detection</name>
<t>
Observability is critical in identifying performance bottlenecks that may arise due to CNI behavior under stress conditions. Benchmarking SHOULD assess the ability of CNIs to expose metrics such as packet drops, queue lengths, or flow counts through standard telemetry interfaces (e.g., Prometheus, OpenTelemetry). Effective bottleneck detection tools and visibility into the data path are essential for root cause analysis. CNIs that provide native observability tooling (e.g., Cilium Hubble) SHOULD be benchmarked for the overhead and fidelity of these features.
</t>
</section>

</section>

<section anchor="CODEF">
<name>Best Practice Operational Example: the CODECO Experimentation Framework</name>
<t>
The CODECO Experimentation Framework (CODEF) is an open-source, modular benchmarking environment developed under the CODECO project to support the evaluation of containerized workloads in edge-to-cloud infrastructures. CODEF adopts a microservice-based architecture to streamline experimentation through abstraction, automation, and reproducibility. CODEF is logically divided into four functional layers, each implemented as an independent containerized microservice. This modular design ensures extensibility and facilitates integration with diverse technologies across the experimentation pipeline. The Infrastructure Manager layer provisions cluster resources across heterogeneous environments, including bare-metal nodes, hypervisor-based virtual machines (e.g., VirtualBox, XCP-ng), and public or academic cloud testbeds (e.g., AWS, CloudLab, EdgeNet).

Following infrastructure allocation, the CODEF Resource Manager deploys software components on each node using parameterized Ansible playbooks. A dedicated instance of the Resource Manager operates per node to guarantee consistent, automated software setup. The Experiment Controller coordinates workload execution, manages experimental iterations, collects measurement data, and invokes benchmarks. The Results Processor performs statistical analysis and post-processing to generate structured outputs, including visualization and reporting artifacts.

CODEF supports full automation of the experimentation lifecycle, from cluster instantiation to metric analysis. Each cluster is provisioned from clean operating system images to ensure consistency, repeatability, and environmental isolation across benchmark runs. This approach eliminates state leakage between tests and enhances comparability. The framework also provides low-level parameterization options for various networking and security configurations. These include tunneling and encapsulation mechanisms (e.g., VXLAN, Geneve, IP-in-IP), encryption protocols (e.g., IPsec, WireGuard), and Linux kernel-based datapath acceleration features (e.g., eBPF and XDP). Such flexibility supports the emulation of production-grade deployments across a wide range of container network interfaces (CNIs) and infrastructure types.
</t>

<figure anchor="fig-codef" title="CODECO Experimentation Framework">
  <artwork><![CDATA[
  +-------------------------------------------+
  |  CODECO Experimentation Framework (CODEF) |
  +-------------------------------------------+
              |
              v
  +------------------------------------+     
  |  Experiment and Cluster Definition |
  +------------------------------------+                                          
              |
              v
  +------------------------+
  |   Experiment Manager   |
  +------------------------+
          |                   Container                Systems
          | Deploy VMs+OS +---------------+     +-------------------+
          +-------------> | Infrastr Mgrs |---> | physical,VM,cloud |
          |               +---------------+     +-------------------+
          | Deploy Resource Managers per node
          |
          |          Containers
          |      +---------------+    +----------+
          |----> | Resource MgrA |<-->|  Master  |      SW / App
          |      +---------------+    +----------+    +---------+
          |----> | Resource MgrB |<-->|  Worker1 |<-->| Ansible |
          |      +---------------+    +----------+    +---------+
          |----> | Resource MgrC |<-->|  WorkerX |
          |      +---------------+    +----------+
          |
          |                   Container
          | Execute Exper +----------------+    +------------+
          +-------------> | Experiment Ctr |<-->| Iteration, |
          |               +----------------+    | Metrics    |
          |                                     +------------+
          |                      Container      
          | Output Results +-------------------+    +-------------+
          +------------->  | Results Processor |<-->| Processing, |
                           +-------------------+    | Stats, LaTeX|
                                                    +-------------+
  ]]></artwork>
</figure>

<section anchor="CODEF-Bench">
<name>CODEF Benchmarking and CNI Support</name>
<t>
CODEF addresses the requirements for repeatable, infrastructure-agnostic benchmarking across the edge-cloud continuum. It supports a wide range of third-party CNI plugins like Antrea, Calico, Cilium, Flannel, Weavenet, Kube-router, Kube-ovn, Multus, and out-of-the-box network solutions like L2S-M [L2S-M]. These CNIs can be evaluated within multiple Kubernetes distributions (e.g., vanilla K8s, K3s, K0s, MicroK8s). Since each plugin employs distinct network layer options (underlay and/or overlay) and exposes different features such as enhanced security or encryption mechanisms, programmable data-paths, or network-policy enforcement, the resulting performance characteristics may vary. CODEF therefore provides a consistent end-to-end methodology for comparing these alternatives under diverse test conditions and across heterogeneous infrastructures and system architectures. Accordingly, any benchmarking methodology SHOULD account for the same dimensions to ensure comparable results.
</t>
</section>

<section anchor="CODEF-Env">
<name>Environment Configuration Aspects</name>
<t>
Besides the distinct characteristics of each CNI implementation, a benchmarking methodology SHOULD also account for architectural and physical variables:

VM/hypervisor or bare-metal deployments, intra- or inter-node scenarios, and
deployments across vanilla or production-grade K8s distributions compared to those designed specifically for the edge.
Hardware heterogeneity adds yet another layer of variability. CPU architecture, core- and thread-counts, cache hierarchy, memory type and speed, NUMA topology, and NIC model or firmware can all vary the results. Likewise, low-level configuration choices including tunnel mode, MTU size, or non-default eBPF parameters can all alter performance outcomes. Experiments conducted with CODEF under a variety of conditions i.e., different intra- and inter-cluster scenarios with different hardware specs, across various K8s distributions, employing different low-level tunneling parameters or when applying high load/stress and workloads, showed significant performance variability. Disparities observed not only among different CNI plugins on the same distribution, but also within a single plugin when used to different K8s distributions. Some key observations showed that deploying lightweight plugins on edge-focused K8s distributions does not automatically translate into better resource efficiency or higher performance. In practice, some plugins downscale their footprint with cost of performance (e.g., by employing a different tunneling protocol), while others incur additional overhead and better network performance on vanilla K8s, a trade-off that should/SHOULD be considered. In addition, the optimal combination of plugin and distribution is highly workload-specific. A particular workload may achieve better results with a configuration that is, on paper, more resource-intensive because it matches the workload’s operational profile. These trade-offs SHOULD be considered, especially in IoT and edge environments, where resource limitations make it imperative to extract maximum performance.
</t>
</section>

<section anchor="CODEF-measurement">
<name>Measurement Tools</name>
<t>
Through Ansible playbooks, CODEF automates the installation of a range of software and tools for both workload generation and measurement. It supports network-centric traffic generators such as iperf3, netperf, and sockperf, as well as comprehensive suites like the K8s-bench-suite, allowing to measure bandwidth, throughput, and packet fragmentation behavior across TCP and UDP protocols and message sizes. Resource footprint is captured at node or container scope for CPU, memory, and disk, with observability stacks built on Prometheus and Grafana to provide live monitoring and historical dashboards. Load and stress tests such as the CNCF Kube-Burner alongside chaos-engineering tools can subject the cluster to scaled pressure, while orchestrated fault scenarios can reveal performance limits and resilience gaps. Power consumption can also be estimated through empirical models or specialized tools such as the CNCF Kepler and Scamander, however, their accuracy SHOULD be considered as it ultimately depends on the underlying hardware counters (e.g., bare-metal, RAPL) and their prediction models especially on virtualized or non-Intel systems.
</t>
</section>

</section>

<section anchor="CODEF-Best-Practices">
<name>Best Practices</name>
<t>
This section outlines best practices for benchmarking Container Network Interface (CNI) plugins in Kubernetes environments, consistent with the principles defined by the IETF Benchmarking Methodology Working Group (BMWG). The focus is on ensuring repeatability, vendor-neutrality, and measurement fidelity in diverse Edge-to-Cloud scenarios.
</t>

<section anchor="CODEF-Test">
<name>Controlled Test Environments</name>
<t>
Benchmarking SHOULD be conducted in isolated testbeds with no extraneous traffic or workloads. The following practices help reduce environmental noise and increase determinism:</t>
<ul>
<li>Use bare-metal or dedicated VMs for benchmarking to avoid cross-tenant interference.</li>
<li>Ensure consistent CPU pinning and disable power-saving features or CPU frequency scaling to stabilize performance measurements.</li>
<li>Synchronize clocks across test nodes using NTP or PTP for accurate latency and jitter measurement.</li>
</ul>
</section>

<section anchor="CODEF-Test1">
<name>Standardized Test Configurations</name>
<t>
Benchmarking SHOULD adhere to pre-defined configurations to enable comparability across CNIs and platforms, aligning with <xref target="RFC2544"/><xref target="RFC6815"/>. The following elements MUST be documented:
</t>
<ul>
<li>Kubernetes version and distribution.</li>
<li>CNI plugin version and configuration parameters.</li>
<li>Kernel version and system tunables (e.g., MTU size, sysctl options).</li>
<li>CPU model, memory size, and network interface type.</li>
</ul>
</section>

<section anchor="CODEF-repeatability">
<name>Test Repeatability and Statistical Significance</name>
<t>
Each experiment SHOULD be repeated a minimum of five times. For latency and throughput metrics, results MUST be reported using:
</t>
<ul>
<li>Minimum, average (median), maximum.</li>
<li>at least 90th, and 95th percentile values.</li>
</ul>

<t>
Furthermore, adequate warm-up times when starting test runs, and cool-down periods between test runs SHOULD be included to prevent thermal bias or residual resource contention. Where possible, automation frameworks (e.g., CODEF, Ansible) SHOULD be used to ensure that each experiment is launched from a clean state.
</t>
</section>

<section anchor="CODEF-traffic">
<name>Traffic Generators, Traffic Models and Load Profiles</name>
<t>
Traffic generators MUST support multiple transport protocols (e.g., TCP, UDP) and varying packet sizes as well as interrarrival packet rates. Benchmarking tools such as iperf3, netperf, and sockperf are RECOMMENDED. For realistic CNI evaluation:
</t>
<ul>
<li>TCP_RR, TCP_CRR, and UDP_RR SHOULD be used to measure latency, jitter, and throughput.</li>
<li>Multiple flows and concurrent connections SHOULD be tested to simulate microservice interactions.</li>
</ul>
<t>
Benchmarks SHOULD include traffic profiles reflecting real-world microservice communications, such as:</t>
<ul>
<li>Short-lived TCP connections (request/response.</li>
<li>Persistent streaming (large payloads, high throughput).</li>
<li>Burst UDP traffic for latency and packet loss analysis.</li>
</ul>
</section>

<section anchor="CODEF-workload-sim">
<name>Workload Simulation, Emulation, and Stress Testing</name>
<t>
To evaluate performance under real-world loads, benchmarking MUST include scenarios with:
</t>
<ul>
<li>Small, average, high pod churn rates (creation/deletion).</li>
<li>Concurrent service access and policy enforcement.</li>
<li>Synthetic network and node failure</li>
</ul>
<t>
Tools such as kube-burner, chaos-mesh, and tc-netem are RECOMMENDED to orchestrate these scenarios, aligning with stress test guidance in <xref target="RFC8239"/>.</t>
</section>


<section anchor="CODEF-observability">
<name>Observability and Resource Instrumentation</name>
<t>
CNIs SHOULD expose internal metrics (e.g., policy hits, flow counts, packet drops). Benchmarks MUST capture:
</t>
<ul>
<li>CPU and memory usage per CNI pod/process via for instance Prometheus.</li>
<li>NIC statistics.</li>
<li>Network path visibility (e.g., using Cilium Hubble or Calico flow logs)</li>
</ul>

<t>
Experimental and open-source examples on how such metrics can be captured at a node and network level can be checked in the CODECO project  <xref target="codeco_d10"/> and respective code <xref target="codeco_d12"/>. Resource metrics MUST be collected at both node-level and pod-level granularity.
</t>
</section>

<section anchor="CODEF-result">
<name>Result Reporting and Output Format</name>
<t>
Benchmarking outputs SHOULD:
</t>
<ul>
<li>Use machine-readable formats (e.g., JSON, YAML, YANG).</li>
<li>Clearly label all test parameters and metrics.</li>
<li>Include system logs, configuration manifests, and tool versions.</li>
</ul>
<t>
A common results schema SHOULD be developed to support comparative analysis and long-term reproducibility, in line with goals in <xref target="RFC6815"/>.</t>
</section>

<section anchor="CODEF-energy">
<name>Energy and Environmental Metrics</name>
<t>
While not core to BMWG benchmarking, energy metrics MAY be collected where relevant. Tools such as Kepler or Scamander MAY be used, but results SHOULD be accompanied by a disclaimer about accuracy limitations in virtualized environments or non-Intel systems.
A related discussion on energy metrics and energy-sensitivity can be found in IETF GREEN, <xref target="draft-ea-ds"/></t>
</section>
</section>

<section anchor="challenges">
<name>Challenges and Future Directions</name>
<t>
- Standardized Output Formats
- Reproducibility Guidelines
- Comparative Analysis Guidelines
</t>
</section>


<section anchor="IANA">
<name>IANA Considerations</name>

<t>This document has no IANA considerations.</t>

</section>

 
<section anchor="security-considerations">
<name>Security Considerations</name>
<t>
Benchmarking tools and automation frameworks may introduce risk vectors such as elevated container privileges or misconfigured network policies. Experiments involving stress tests or fault injection should be performed in isolated environments. Benchmarking outputs SHOULD NOT expose sensitive cluster configuration or node-level details.
</t>

</section>

  </middle>
  
  

  <back>
    <references>
      <name>References</name>
      <references>
        <name>Normative References</name>
        
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.2119.xml"/>
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.8174.xml"/>
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.7312.xml"/> 
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.2285.xml"/>
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.2544.xml"/> 
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.1242.xml"/>
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.8172.xml"/> 
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.6808.xml"/>
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.8239.xml"/> 
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.6815.xml"/>
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.5481.xml"/>
        <xi:include href="https://bib.ietf.org/public/rfc/bibxml/reference.RFC.9315.xml"/>
  
        <!-- The recommended and simplest way to include a well known reference -->
        
      </references>
 
      <references>
        <name>Informative References</name>
     
        <reference anchor="codef">
        <!-- [REPLACE/DELETE] Example minimum reference -->
          <front>
            <title>"CODECO Experimental Framework, https://gitlab.eclipse.org/eclipse-research-labs/codeco-project/experimentation-framework-and-demonstrations/experimentation-framework </title>
            <author initials="G." surname="Koukis et al.">
              <organization/>
            </author>
            <date year="2024"/>
            <!-- [CHECK] -->
          </front>
        </reference>
        
        <reference anchor="codeco_d12">
        <!-- [REPLACE/DELETE] Example minimum reference -->
          <front>
            <title>"CODECO D12 - Basic Operation Components and Toolkit version 2.0. Zenodo. https://doi.org/10.5281/zenodo.12819424" </title>
            <author initials="G." surname="Samaras et al.">
              <organization/>
            </author>
            <date year="2024"/>
            <!-- [CHECK] -->
          </front>
        </reference>
        
        <reference anchor="draft-ea-ds">
        <!-- [REPLACE/DELETE] Example minimum reference -->
          <front>
            <title>"Energy-aware Differentiated Services (EA-DS). IETF draft draft-sofia-green-energy-aware-diffserv-00, active,  https://datatracker.ietf.org/doc/draft-sofia-green-energy-aware-diffserv/." </title>
            <author initials="R." surname="C. Sofia et al.">
              <organization/>
            </author>
            <date year="2025"/>
            <!-- [CHECK] -->
          </front>
        </reference>
        
       <reference anchor="codeco_d10" target="https://doi.org/10.5281/zenodo.12819444">
  <front>
    <title>CODECO Deliverable D10: Technological Guidelines, Reference Architecture, and Open-source Ecosystem Design</title>
    <author initials="R." surname="Sofia" fullname="Rute C. Sofia"/>
    <author>
      <organization>CODECO Consortium</organization>
    </author>
    <date year="2024"/>
  </front>
  <seriesInfo name="CODECO" value="D10"/>
</reference>
        
        <reference anchor="ietf-bmwg-07">
        <!-- [REPLACE/DELETE] Example minimum reference -->
          <front>
            <title>"Considerations for Benchmarking Network Performance in Containerized Infrastructures, draft-ietf-bmwg-containerized-infra-07,  active, https://datatracker.ietf.org/doc/draft-ietf-bmwg-containerized-infra/07/." </title>
            <author initials="T." surname="Ngoc et al.">
              <organization/>
            </author>
            <date year="2025"/>
            <!-- [CHECK] -->
          </front>
        </reference>
  
        <reference anchor="antrea">
        <front>
        <title>Antrea CNI</title>
        <author>
        <organization>Antrea Project, https://antrea.io</organization>
        </author>
        <date year="2024"/>
        </front>
        </reference>

       <reference anchor="calico">
         <front>
          <title>Project Calico, https://www.tigera.io/project-calico/</title>
          <author>
             <organization>Tigera, Inc.</organization>
          </author>
          <date year="2024"/>
      </front>

      </reference>

<reference anchor="cilium">
  <front>
    <title>Cilium: eBPF-based Networking, Security, and Observability, https://cilium.io</title>
    <author>
      <organization>Isovalent</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="flannel">
  <front>
    <title>Flannel CNI Plugin, https://github.com/flannel-io/flannel</title>
    <author>
      <organization>flannel-io</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="kube-ovn">
  <front>
    <title>Kube-OVN: A Cloud-Native SDN for Kubernetes, https://github.com/kubeovn/kube-ovn</title>
    <author>
      <organization>Kube-OVN Project</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="kube-router">
  <front>
    <title>Kube-Router: All-in-One CNI, Service Proxy, and Network Polic, https://github.com/cloudnativelabs/kube-routery</title>
    <author>
      <organization>Kube-Router Community</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="weavenet">
  <front>
    <title>Weave Net: Fast, Simple Networking for Kubernetes, https://github.com/weaveworks/weave</title>
    <author>
      <organization>Weaveworks (archived)</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="K8s-netw-model">
  <front>
    <title>Kubernetes Networking Concepts, https://kubernetes.io/docs/concepts/cluster-administration/networking/</title>
    <author>
      <organization>Kubernetes</organization>
    </author>
    <date year="2024"/>
  </front>
 </reference>

<reference anchor="cilium-bench">
  <front>
    <title>Cilium Benchmarking Tools, https://docs.cilium.io/en/latest/operations/performance/</title>
    <author>
      <organization>Isovalent</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="TNSM21-cni">
  <front>
    <title>Benchmarking Kubernetes Container Network Interfaces: Methodology, Metrics, and Observations, https://arxiv.org/abs/2401.07674</title>
    <author initials="G." surname="Koukis et al.">
   </author>
    <date year="2024" month="January"/>
  </front>
</reference>

<reference anchor="aws-vpc-cni-docs" target="https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html">
  <front>
    <title>Amazon EKS Pod Networking with the AWS VPC CNI</title>
    <author>
      <organization>Amazon Web Services</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="prometheus-docs" target="https://prometheus.io/docs/introduction/overview/">
  <front>
    <title>Prometheus Monitoring System Overview</title>
    <author>
      <organization>Prometheus Authors</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="cadvisor-docs" target="https://github.com/google/cadvisor">
  <front>
    <title>cAdvisor: Container Advisor</title>
    <author>
      <organization>Google</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="tc-netem" target="https://man7.org/linux/man-pages/man8/tc-netem.8.html">
  <front>
    <title>tc-netem: Network Emulation</title>
    <author>
      <organization>Linux Foundation</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="kube-burner" target="https://github.com/cloud-bulldozer/kube-burner">
  <front>
    <title>Kube-Burner: Kubernetes Performance and Scalability Tool</title>
    <author>
      <organization>Cloud-Bulldozer Project</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="iperf3" target="https://iperf.fr/">
  <front>
    <title>iPerf3: Network Bandwidth Measurement Tool</title>
    <author>
      <organization>ESnet / Lawrence Berkeley National Lab</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>

<reference anchor="k6" target="https://k6.io/docs/">
  <front>
    <title>k6: Modern Load Testing Tool</title>
    <author>
      <organization>Grafana Labs</organization>
    </author>
    <date year="2024"/>
  </front>
</reference>


      </references>
    </references>
    
       <section anchor="Acknowledgements" numbered="false">
      <!-- [REPLACE/DELETE] an Acknowledgements section is optional -->
      <name>Acknowledgements</name>
      <t>
      This work has been funded by The European Commission in the context of the Horizon Europe CODECO project under grant number 101092696, and by SGC, Grant agreement nr: M-0626, project SemComIIoT. 
      We thank the following colleagues for their discussions concerning network probing and exposure as well as application workload scheduling aspects:
      </t>
      <ul>
      <li>Alberto del Rio, Universidad Politecnica de Madrid, for contributions for active probing.</li>
      <li>Kaikang Huang, fortiss, for contributions on energy-efficient network telemetry and probing</li>
      <li>Tina Samizadeh, fortiss, for contributions on scheduling benchmarking</li>
      <li>Alejandro Muniz, Telefonica, for contributions on network exposure</li>
      <li>Luis Miguel Contreras Murillo, Telefonica, for contributions on network exposure</li>
      </ul>
    </section>
    
    
 </back>
</rfc>

